{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad05583b-2fd0-44cc-8126-237e81f18a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# reading csv file \n",
    "df = pd.read_csv(\"Sample_Req.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a99e700-468b-4048-bf19-ee917ce3807b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"GROQ_API_KEY\"] = \"YOUR_API_KEY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cafacbbe-8951-4820-9265-024678c2f494",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "llm = ChatGroq(\n",
    "    model=\"llama-3.3-70b-versatile\",\n",
    "    temperature=0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6fb1817-13f5-4e49-a443-6a4858f3e2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "template = '''\n",
    "You are given a pandas data frame. You need to convert the data frame into JSON object.\n",
    "1. Return a valid JSON. No preamble. \n",
    "2. The JSON object's keys should correspond to the column headers of the data frame, and each row should become a JSON object with its respective column values\n",
    "\n",
    "Here is the data frame on which you need to perform this task:  \n",
    "{data_frame}\n",
    "'''\n",
    "\n",
    "pt = PromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72d30d75-6bf1-4afa-ac63-ae4982e4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = pt | llm\n",
    "response = chain.invoke(input={\"data_frame\": df})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3e82a8e7-f83c-4486-a0ef-be3d066b5be1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.exceptions import OutputParserException\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "\n",
    "try:\n",
    "    json_parser = JsonOutputParser()\n",
    "    res = json_parser.parse(response.content)\n",
    "except OutputParserException:\n",
    "    raise OutputParserException(\"Context too big. Unable to parse jobs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d4e3cc0-23dd-4f3a-a7be-6ef2def7e959",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "embeddings_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "\n",
    "documents = [(Document(page_content=str(obj))) for obj in res]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "352b671a-b4b0-4b95-887c-885846132c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "from langchain_chroma import Chroma\n",
    "\n",
    "# ids = [str(uuid.uuid4()) for _ in range(len(documents))] \n",
    "\n",
    "unique_collection_name = f\"collection_{uuid.uuid4()}\"\n",
    "\n",
    "db = Chroma.from_documents(documents, embeddings_model, collection_name=unique_collection_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e9481ae8-30a7-4268-916f-1882f0d5aaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_documents = db.get()[\"documents\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "df6af17d-14d9-4337-b81d-765b4e0461f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "from langchain_core.tools import tool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Define the input schema\n",
    "class RequirementInput(BaseModel):\n",
    "    ID: str = Field(..., description=\"The id of the requirement, e.g., 421578\")\n",
    "    Primary_Text: str = Field(..., description=\"The primary text describing the requirement.\")\n",
    "    Verification_Criteria: str = Field(..., description=\"The verification criteria for the requirement.\")\n",
    "    Artifact_Type: str = Field(..., description=\"The type of requirement, e.g., 'SYS Requirement'.\")\n",
    "\n",
    "@tool\n",
    "def write_test_case(requirement: RequirementInput) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "       Generate Test Cases by dynamically creating 'Steps to Execute' using an LLM.\n",
    "    \"\"\"\n",
    "    # Ensure the artifact type is correct for test case generation\n",
    "    if artifact_type != \"SYS Requirement\":\n",
    "        pass\n",
    "        \n",
    "    # # Extract relevant details from the requirement\n",
    "    # req_ID = requirement.get(\"id\", \"No id mentioned\")\n",
    "    # primary_text = requirement.get(\"Primary Text\", \"No primary text provided.\")\n",
    "    # verification_criteria = requirement.get(\"Verification Criteria\", \"No verification criteria provided.\")\n",
    "    # artifact_type = requirement.get(\"Artifact Type\", \"\")\n",
    "\n",
    "    steps_prompt = (\n",
    "        f\"Based on the following requirement, write detailed steps to execute for testing:\\n\\n\"\n",
    "        f\"Primary Text: {requirement.Primary_Text}\\n\"\n",
    "        f\"Verification Criteria: {requirement.Verification_Criteria}\\n\\n\"\n",
    "        f\"Steps to Execute should be clear, actionable, and cover all necessary validation scenarios.\"\n",
    "    )\n",
    "\n",
    "    steps_to_execute = llm.invoke(steps_prompt).strip()\n",
    "\n",
    "    expected_op_prompt = (\n",
    "        f\"For the each value of {steps_to_execute}, write its corresponding expected behaviour.\"\n",
    "        f\"Expected behaviour is to verify the input steps are correcting performed or not.\"\n",
    "        '''Example format\n",
    "        Input: \n",
    "        1. Set up the test environment to simulate Under Voltage.\n",
    "        2. Verify the iPDM provides full functionality during Under Voltage.\n",
    "        Expected Output:\n",
    "        1. Under Voltage should be simulated\n",
    "        2. iPDM should provide full functionality during Under Voltage'''\n",
    "    )\n",
    "\n",
    "    expected_op = llm.invoke(expected_op_prompt).strip()\n",
    "\n",
    "    # Generate a test case based on the primary text\n",
    "    test_case = {\n",
    "        \"Test Case ID\": f\"TC_{hash(requirement.Primary_Text) % 10000}\",\n",
    "        \"Requirement Covered\": requirement.id,\n",
    "        \"Objective\": f\"Validate: {requirement.Primary_Text}\",\n",
    "        \"Pre-conditions\": \"1. The system is powered on and operational.\\n\"\n",
    "            \"2. The system is in a known state before test execution.\",\n",
    "        \"Steps to Execute\": steps_to_execute,\n",
    "        \"Expected Results\": expected_op\n",
    "    }\n",
    "    return test_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1362569c-97b5-443d-b9f0-a5d5d59f6379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from langchain.prompts import PromptTemplate\n",
    "\n",
    "# template = \"\"\" Role and Objective: You are an AI Test Engineer specializing in the automotive industry. Your primary responsibility is to analyze system requirements stored in a vector store, understand their context, and write detailed test cases to validate these requirements.\n",
    "\n",
    "# How You Work:\n",
    "\n",
    "#     Requirement Analysis:\n",
    "#         Access and retrieve requirements stored in the vector store. Each requirement is in JSON format, containing fields such as:\n",
    "#             Primary Text: Describes the main functionality or behavior.\n",
    "#             Verification_Criteria: Specifies the conditions to validate the requirement.\n",
    "#             Artifact Type: Indicates the type of requirement (e.g., \"SYS Requirement\"). Test cases should only be written for system requirements (Artifact Type = \"SYS Requirement\").\n",
    "\n",
    "#     Context Understanding:\n",
    "#         Analyze the Primary Text and the Verification_Criteria to understand the requirement's intent and scope fully.\n",
    "#         Consider automotive industry standards, safety-critical elements, and typical use cases when interpreting requirements.\n",
    "\n",
    "#     Test Case Creation:\n",
    "#         Use the write_test_case tool to generate structured, clear, and concise test cases.\n",
    "#         Ensure test cases:\n",
    "#             Align with the Verification_Criteria.\n",
    "#             Are robust and cover edge cases.\n",
    "#             Follow industry-standard formats and nomenclature.\n",
    "\n",
    "# Output Format:\n",
    "# Provide test cases in a structured format. Each test case should include:\n",
    "\n",
    "#     Test Case ID\n",
    "#     Test Case Title\n",
    "#     Objective\n",
    "#     Pre-conditions\n",
    "#     Steps\n",
    "#     Expected Results\n",
    "\n",
    "# Constraints:\n",
    "\n",
    "#     Generate test cases only for requirements where Artifact Type is \"SYS Requirement\".\n",
    "#     Ensure the test cases are domain-specific, practical, and implementable. \n",
    "\n",
    "# Example Workflow:\n",
    "\n",
    "#     Retrieve and analyze the following requirement:\n",
    "\n",
    "#     {\n",
    "#       \"Primary Text\": \"The system shall automatically lock all doors when the vehicle speed exceeds 10 km/h.\",\n",
    "#       \"Verification_Criteria\": \"Verify that the doors lock when the vehicle speed crosses 10 km/h, and unlock only when the vehicle is stationary.\",\n",
    "#       \"Artifact Type\": \"SYS Requirement\"\n",
    "#     }\n",
    "\n",
    "#     Understand the context and functionality.\n",
    "    \n",
    "#     Write a corresponding test case:\n",
    "#         Test Case ID: TC_001\n",
    "#         Test Case Title: Door Lock Activation at Speed\n",
    "#         Objective: Validate that the doors automatically lock when vehicle speed exceeds 10 km/h.\n",
    "#         Pre-conditions: Vehicle is stationary; doors are unlocked.\n",
    "#         Steps:\n",
    "#         1. Start the vehicle and gradually increase the speed to 11 km/h.\n",
    "#         2. Observe the door lock status.\n",
    "#         3. Bring the vehicle to a complete stop.\n",
    "#         4. Check the door lock status.\n",
    "#         Expected Results:\n",
    "#         1. At 11 km/h, all doors should lock automatically.\n",
    "#         2. When the vehicle is stationary, doors should remain locked until manually unlocked.\n",
    "\n",
    "# Key Qualities:\n",
    "\n",
    "#     Act like a highly experienced test engineer with a deep understanding of automotive systems.\n",
    "#     Be precise, systematic, and focused on producing high-quality test cases. \"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "795860a0-1906-4931-839f-d66045b3f988",
   "metadata": {},
   "outputs": [],
   "source": [
    "def writetestcasetool(primary_text, verification_criteria,id):\n",
    "    test_case_id = \"TC_\" + str(hash(primary_text))[:6]  # Generate a unique test case ID\n",
    "    test_case = {\n",
    "        \"Test Case ID\": test_case_id,\n",
    "        \"Requirement Coverd\": id,\n",
    "        \"Test Case Title\": f\"Test for {primary_text[:30]}...\",  # Shortened primary text as title\n",
    "        \"Objective\": f\"Validate that the system meets the requirement: {primary_text}\",\n",
    "        \"Pre-conditions\": \"Define the system's initial state required for the test.\",\n",
    "        \"Steps\": [\n",
    "            f\"Step 1: {verification_criteria.split(',')[0].strip()}\",\n",
    "            \"Step 2: Additional steps based on requirement.\"\n",
    "        ],\n",
    "        \"Expected Results\":  (\n",
    "            f\"Each test step is performed correctly, and the outcomes match the requirement's intent: {primary_text}. \"\n",
    "            f\"Confirm proper system behavior as per verification criteria: {verification_criteria}.\"\n",
    "        )\n",
    "    }\n",
    "    return json.dumps(test_case, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "30a372b5-43e1-41b6-a79b-d79c644b7aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_prompt_template = PromptTemplate(\n",
    "    #input_variables=[\"requirement_json\", \"agent_scratchpad\"],\n",
    "    template=(\n",
    "       \"You are an AI Test Engineer for the automotive industry.\\n\\n\"\n",
    "        \"Here is the system requirement to analyze:\\n{input}\\n\\n\"\n",
    "        \"Follow this format:\\n\"\n",
    "        \"Thought: Describe your next step in analyzing the requirement.\\n\"\n",
    "        \"Action: Indicate the tool you will use (e.g., write_test_case).\\n\"\n",
    "        \"Action_Input: Provide the input to the tool.\\n\"\n",
    "        \"Observation: Record the result of the action.\\n\\n\"\n",
    "        \"Continue reasoning and repeat until the task is complete. Then provide the final test case.\\n\\n\"\n",
    "        \"Current inputs:\\n{input}\\n\\n\"\n",
    "        \"{agent_scratchpad}\"\n",
    "        \n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "76f2b68e-b229-46a8-a739-4ecafbb749f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# llm_chain = agent_prompt_template | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2f565adb-e7e8-4bda-a7fd-7f980012ec14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "memory = ConversationBufferMemory(return_messages=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7527b8e7-1abb-4375-b7bc-3d81d2ac1575",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import initialize_agent, Tool\n",
    "\n",
    "# Define the tool for writing test cases\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"write_test_case\",\n",
    "        description=\"Generate or modify structured test cases for system requirements.\",\n",
    "        func=lambda input: write_test_case(\n",
    "        input[\"primary_text\"], input[\"verification_criteria\"], input[\"id\"]\n",
    "        )\n",
    "    )\n",
    "]\n",
    "    \n",
    "# Initialize the agent\n",
    "test_engineer_agent = initialize_agent(\n",
    "        tools,\n",
    "        llm,\n",
    "        prompt = agent_prompt_template,\n",
    "        memory=memory, # Store interaction contex\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b5353cb7-b1bd-47b7-966e-aa9abc1e886e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent task execution\n",
    "import json\n",
    "\n",
    "def execute_test_case_generation(requirement):\n",
    "    # requirement_json = json.dumps(requirement, indent=2)\n",
    "    # input_data = {\"requirement_json\":  requirement, \n",
    "    #               \"agent_scratchpad\": \"\", \n",
    "    #         }\n",
    "    return test_engineer_agent.invoke({\"input\":requirement})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "030de6f8-d2b8-4479-b722-c4642c350e26",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "`run` not supported when there is not exactly one output key. Got [].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[112], line 16\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# input_data = {\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m#     \"requirement_json\": json.dumps(requirement, indent=2),\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#     \"agent_scratchpad\": \"\"  # Initialize scratchpad as empty\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# }\u001b[39;00m\n\u001b[0;32m     15\u001b[0m agent_executor \u001b[38;5;241m=\u001b[39m AgentExecutor(agent\u001b[38;5;241m=\u001b[39mtest_engineer_agent, tools\u001b[38;5;241m=\u001b[39mtools, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, handle_parsing_error\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m---> 16\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43magent_executor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrequirement\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m pprint(response)\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# test_cases = execute_test_case_generation(str(requirement))\u001b[39;00m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m# print(test_cases)\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain_core\\_api\\deprecation.py:182\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    180\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    181\u001b[0m     emit_warning()\n\u001b[1;32m--> 182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:601\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Convenience method for executing chain.\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \n\u001b[0;32m    566\u001b[0m \u001b[38;5;124;03mThe main difference between this method and `Chain.__call__` is that this\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m        # -> \"The temperature in Boise is...\"\u001b[39;00m\n\u001b[0;32m    599\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    600\u001b[0m \u001b[38;5;66;03m# Run at start to make sure this is possible/defined\u001b[39;00m\n\u001b[1;32m--> 601\u001b[0m _output_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_output_key\u001b[49m\n\u001b[0;32m    603\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs:\n\u001b[0;32m    604\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:549\u001b[0m, in \u001b[0;36mChain._run_output_key\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_output_key\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m    548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 549\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    550\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` not supported when there is not exactly \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    551\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mone output key. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    552\u001b[0m         )\n\u001b[0;32m    553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_keys[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mValueError\u001b[0m: `run` not supported when there is not exactly one output key. Got []."
     ]
    }
   ],
   "source": [
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "requirement = {\n",
    "    \"id\": \"214542\",\n",
    "    \"Primary Text\": \"The system shall automatically lock all doors when the vehicle speed exceeds 10 km/h\",\n",
    "    \"Verification_Criteria\": \"Verify that the doors lock when the vehicle speed crosses 10 km/h, and unlock only when the vehicle is stationary.\",\n",
    "    \"Artifact Type\": \"SYS Requirement\"\n",
    "}\n",
    "\n",
    "# input_data = {\n",
    "#     \"requirement_json\": json.dumps(requirement, indent=2),\n",
    "#     \"agent_scratchpad\": \"\"  # Initialize scratchpad as empty\n",
    "# }\n",
    "\n",
    "agent_executor = AgentExecutor(agent=test_engineer_agent, tools=tools, verbose=True, handle_parsing_error=True)\n",
    "response = agent_executor.run(input=requirement)\n",
    "\n",
    "pprint(response)\n",
    "\n",
    "# test_cases = execute_test_case_generation(str(requirement))\n",
    "# print(test_cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4e8075c1-b9b3-4d1b-b275-261aeab4e7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(requirement)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c03471-0649-4d4a-abd7-6c09cddc434c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
